version: 0.2

phases:
  install:
    runtime-versions:
      python: 3.x
    commands:
      - pip install --upgrade pip
      - pip install requests
  build:
    commands:
      - echo "Downloading DAG file from S3"
      - aws s3 cp s3://your-bucket/your_dag.py ./your_dag.py

      - echo "Creating the Python script inline"
      - |
        cat <<EOF > update_job_ids.py
        import sys
        import requests
        import re

        def list_jobs():
            # Hardcoded API endpoint and token
            databricks_host = 'https://your-databricks-workspace-url'  # Replace with your Databricks URL
            databricks_token = 'your-databricks-token'                 # Replace with your Databricks token
            headers = {'Authorization': f'Bearer {databricks_token}'}
            response = requests.get(f'{databricks_host}/api/2.1/jobs/list', headers=headers)
            response.raise_for_status()
            jobs = response.json()
            return jobs

        def update_job_id_in_dag(filepath, jobs_data):
            with open(filepath, 'r') as file:
                lines = file.readlines()
            task_pattern = re.compile(r"task_id='run_(.*?)'")
            job_id_pattern = re.compile(r'job_id=.*?,')
            for i, line in enumerate(lines):
                task_match = task_pattern.search(line)
                if task_match:
                    job_name = task_match.group(1).strip()
                    job_id = None
                    for job in jobs_data.get('jobs', []):
                        if job['settings']['name'] == job_name:
                            job_id = job['job_id']
                            break
                    if not job_id:
                        error_message = f"{job_name} does not exist"
                        print(error_message)
                        sys.exit(1)
                    # Update the job_id in the next line
                    for j in range(i + 1, min(i + 5, len(lines))):
                        if 'job_id=' in lines[j]:
                            lines[j] = job_id_pattern.sub(f'job_id={job_id},', lines[j])
                            break
            with open(filepath, 'w') as file:
                file.writelines(lines)

        def main():
            jobs_data = list_jobs()
            update_job_id_in_dag('./your_dag.py', jobs_data)

        if __name__ == '__main__':
            main()
        EOF

      - echo "Running the Python script"
      - python update_job_ids.py

      - echo "Uploading updated DAG file back to S3"
      - aws s3 cp ./your_dag.py s3://your-bucket/updated_your_dag.py

post_build:
  commands:
    - echo "Build completed successfully."
