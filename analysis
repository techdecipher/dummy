version: 0.2
phases:
  install:
    runtime-versions:
      python: 3.x

 
  build:
    commands:
      
      - cd $CODEBUILD_SRC_DIR_source_artifactsDbr || exit 1
      - echo "reading params file"
      - PARAM_FILE="parameters.json"
      - modified_dags_files=$(jq -r '.modified_dags_files[]' "$PARAM_FILE");
      - modified_scripts_files=$(jq -r '.modified_scripts_files[]' "$PARAM_FILE");
      - echo "Fetching the latest changes from the repository"
      - cd dags/databricks/FDL/dags
      - echo "Defining modified files manually"
      - echo "$modified_dags_files"
      - echo "$modified_scripts_files"
      - |
        if [ "$modified_dags_files" ] && [ "$modified_scripts_files" ]; then
            echo "modified files under both dags and scripts env variable is active"
            echo "first processing dags files"
            echo "$modified_dags_files"
            for python_file in $modified_dags_files; do
              echo "Processing file: $python_file"
 
              # Extract task_id lines from the current Python file
              jobNames=$(grep -n "task_id='run_fdl_dbjob_" "$python_file" || true)
 
              # Write task line numbers and task names into output.txt
              while IFS= read -r line; do
                lineNumber=$(echo "$line" | cut -d: -f1)
                jobName=$(echo "$line" | cut -d: -f2- | sed "s/task_id='run_//" | sed "s/',//" | xargs)
                echo "$lineNumber $jobName" >> output.txt
              done <<< "$jobNames"
 
              # Process each task from output.txt
              echo "Processing tasks from output.txt"
              while IFS= read -r entry; do
                lineNumber=$(echo "$entry" | awk '{print $1}')
                task_name=$(echo "$entry" | awk '{print $2}')
                if [ "$task_name" ] && [ "$lineNumber" ]; then
                
                     echo "Processing task_name: $task_name on line $lineNumber"
                     curl   --connect-timeout 30 --retry 3 --retry-delay 5 \
                        --request GET "URL/api/2.1/jobs/list?name=${task_name}" \
                        --header "Authorization: Bearer TOKEN" > "apiresponse.json"
 
                     job_value=$(jq -r '.jobs[0].job_id // empty' "apiresponse.json")
                     job_name=$(jq -r '.jobs[0].settings.name // empty' "apiresponse.json")
                     if [ -z "$job_value" ] || [ -z "$job_name" ]; then
                          echo "Error: Missing job details for task_name: $task_name"
                          continue
                     fi
 
                    echo "Job Value: $job_value"
                    echo "Task Value: $job_name"
                     if [ "$task_name" == "$job_name" ]; then
                         echo "Updating job_id for $task_name on line $lineNumber in $python_file"
                         awk -v lineNum="$lineNumber" -v jobVal="$job_value" '
                         NR==lineNum+1 { sub(/job_id[[:space:]]*=.*/, "job_id=" jobVal ","); }
                         { print }
                         ' "$python_file" > temp.py && mv temp.py "$python_file"
                     else
                         echo "No match found for $task_name and $job_name"
                     fi
                else
                    echo "There is no job name that exists in DBR application for this $python_file file"
                fi
              done < output.txt
              echo "Starting sync"
              pwd
              #if [ -f "apiresponse.json" ]; then rm apiresponse.json; fi
              #if [ -f "output.txt" ]; then rm output.txt; fi
              aws s3 sync . s3://testairflows3poc/dags/databricks/FDL/dags --exclude "*" --include "$python_file"
            done
            
            ls -ltrh
           
            echo "now processing scripts files"
            cd ..
            pwd
            cd scripts
            pwd
            echo "Script section is running"
            echo "$modified_scripts_files"
            for script_python_file in $modified_scripts_files; do
              echo "Processing file: $script_python_file"
 
              # Extract task_id lines from the current Python file
              jobNames=$(grep -n "task_id='run_fdl_dbjob_" "$script_python_file" || true)
              #this output file contains data from dags so clearing it here for fresh start for script side
              > output.txt
              while IFS= read -r line; do
                lineNumber=$(echo "$line" | cut -d: -f1)
                jobName=$(echo "$line" | cut -d: -f2- | sed "s/task_id='run_//" | sed "s/',//" | xargs)
                echo "$lineNumber $jobName" >> output.txt
              done <<< "$jobNames"
 
              # Process each task from output.txt
              echo "Processing tasks from output.txt"
              while IFS= read -r entry; do
                lineNumber=$(echo "$entry" | awk '{print $1}')
                task_name=$(echo "$entry" | awk '{print $2}')

                if [ "$task_name" ] && [ "$lineNumber" ]; then
                
                     echo "Processing task_name: $task_name on line $lineNumber"
                     curl   --connect-timeout 30 --retry 3 --retry-delay 5 \
                        --request GET "URL/api/2.1/jobs/list?name=${task_name}" \
                        --header "Authorization: Bearer TOKEN" > "apiresponse.json"
 
                     job_value=$(jq -r '.jobs[0].job_id // empty' "apiresponse.json")
                     job_name=$(jq -r '.jobs[0].settings.name // empty' "apiresponse.json")
                     if [ -z "$job_value" ] || [ -z "$job_name" ]; then
                          echo "Error: Missing job details for task_name: $task_name"
                          continue
                     fi
 
                    echo "Job Value: $job_value"
                    echo "Task Value: $job_name"
                     if [ "$task_name" == "$job_name" ]; then
                         echo "Updating job_id for $task_name on line $lineNumber in $script_python_file"
                         awk -v lineNum="$lineNumber" -v jobVal="$job_value" '
                         NR==lineNum+1 { sub(/job_id[[:space:]]*=.*/, "job_id=" jobVal ","); }
                         { print }
                         ' "$script_python_file" > temp.py && mv temp.py "$script_python_file"
                     else
                         echo "No match found for $task_name and $job_name"
                     fi
                else
                    echo "There is no job name that exists in DBR application for this $script_python_file file"
                fi
              done < output.txt
              #if [ -f "apiresponse.json" ]; then rm apiresponse.json; fi
              #if [ -f "output.txt" ]; then rm output.txt; fi
              aws s3 sync . s3://testairflows3poc/dags/databricks/FDL/scripts --exclude "*" --include "$script_python_file"
            done
            pwd
            
            
        elif [  "$modified_dags_files" ]; then
            echo "modified files under dags only active"
    
            echo "$modified_dags_files"
     
            # Loop through each Python file and perform the tasks
            echo "Processing each Python file"
      
            for python_file in $modified_dags_files; do
              echo "Processing file: $python_file"
 
              # Extract task_id lines from the current Python file
              jobNames=$(grep -n "task_id='run_fdl_dbjob_" "$python_file" || true)
 
              # Write task line numbers and task names into output.txt
              while IFS= read -r line; do
                lineNumber=$(echo "$line" | cut -d: -f1)
                jobName=$(echo "$line" | cut -d: -f2- | sed "s/task_id='run_//" | sed "s/',//" | xargs)
                echo "$lineNumber $jobName" >> output.txt
              done <<< "$jobNames"
 
              # Process each task from output.txt
              echo "Processing tasks from output.txt"
              while IFS= read -r entry; do
                lineNumber=$(echo "$entry" | awk '{print $1}')
                task_name=$(echo "$entry" | awk '{print $2}')
                if [ "$task_name" ] && [ "$lineNumber" ]; then
                
                     echo "Processing task_name: $task_name on line $lineNumber"
                     curl   --connect-timeout 30 --retry 3 --retry-delay 5 \
                        --request GET "URL/api/2.1/jobs/list?name=${task_name}" \
                        --header "Authorization: Bearer TOKEN" > "apiresponse.json"
 
                     job_value=$(jq -r '.jobs[0].job_id // empty' "apiresponse.json")
                     job_name=$(jq -r '.jobs[0].settings.name // empty' "apiresponse.json")
                     if [ -z "$job_value" ] || [ -z "$job_name" ]; then
                          echo "Error: Missing job details for task_name: $task_name"
                          continue
                     fi
 
                    echo "Job Value: $job_value"
                    echo "Task Value: $job_name"
                     if [ "$task_name" == "$job_name" ]; then
                         echo "Updating job_id for $task_name on line $lineNumber in $python_file"
                         awk -v lineNum="$lineNumber" -v jobVal="$job_value" '
                         NR==lineNum+1 { sub(/job_id[[:space:]]*=.*/, "job_id=" jobVal ","); }
                         { print }
                         ' "$python_file" > temp.py && mv temp.py "$python_file"
                     else
                         echo "No match found for $task_name and $job_name"
                     fi
                else
                    echo "There is no job name that exists in DBR application for this $python_file file"
                fi
              done < output.txt
              #if [ -f "apiresponse.json" ]; then rm apiresponse.json; fi
              #if [ -f "output.txt" ]; then rm output.txt; fi
              aws s3 sync . s3://testairflows3poc/dags/databricks/FDL/dags --exclude "*" --include "$python_file"
            done
    
        elif [  "$modified_scripts_files" ]; then
            cd ..
            cd scripts
            echo "Script section is running"
            for python_file in $modified_scripts_files; do
              echo "Processing file: $python_file"
 
              # Extract task_id lines from the current Python file
              jobNames=$(grep -n "task_id='run_fdl_dbjob_" "$python_file" || true)
              > output.txt
              while IFS= read -r line; do
                lineNumber=$(echo "$line" | cut -d: -f1)
                jobName=$(echo "$line" | cut -d: -f2- | sed "s/task_id='run_//" | sed "s/',//" | xargs)
                echo "$lineNumber $jobName" >> output.txt
              done <<< "$jobNames"
 
              # Process each task from output.txt
              echo "Processing tasks from output.txt"
              while IFS= read -r entry; do
                lineNumber=$(echo "$entry" | awk '{print $1}')
                task_name=$(echo "$entry" | awk '{print $2}')

                if [ "$task_name" ] && [ "$lineNumber" ]; then
                
                     echo "Processing task_name: $task_name on line $lineNumber"
                     curl   --connect-timeout 30 --retry 3 --retry-delay 5 \
                        --request GET "URL/api/2.1/jobs/list?name=${task_name}" \
                        --header "Authorization: Bearer TOKEN" > "apiresponse.json"
 
                     job_value=$(jq -r '.jobs[0].job_id // empty' "apiresponse.json")
                     job_name=$(jq -r '.jobs[0].settings.name // empty' "apiresponse.json")
                     if [ -z "$job_value" ] || [ -z "$job_name" ]; then
                          echo "Error: Missing job details for task_name: $task_name"
                          continue
                     fi
 
                    echo "Job Value: $job_value"
                    echo "Task Value: $job_name"
                     if [ "$task_name" == "$job_name" ]; then
                         echo "Updating job_id for $task_name on line $lineNumber in $python_file"
                         awk -v lineNum="$lineNumber" -v jobVal="$job_value" '
                         NR==lineNum+1 { sub(/job_id[[:space:]]*=.*/, "job_id=" jobVal ","); }
                         { print }
                         ' "$python_file" > temp.py && mv temp.py "$python_file"
                     else
                         echo "No match found for $task_name and $job_name"
                     fi
                else
                    echo "There is no job name that exists in DBR application for this $python_file file"
                fi
              done < output.txt
              #if [ -f "apiresponse.json" ]; then rm apiresponse.json; fi
              #if [ -f "output.txt" ]; then rm output.txt; fi
              aws s3 sync . s3://testairflows3poc/dags/databricks/FDL/scripts --exclude "*" --include "$python_file"
            done
      
        else
            echo "Runing for all py files at dags directory"
            python_files=$(find . -name "*.py")
 
            #Loop through each Python file and perform the tasks
            echo "Processing each Python file"
            for python_file in $python_files; do
              echo "Processing file: $python_file"
 
              # Extract task_id lines from the current Python file
              jobNames=$(grep -n "task_id='run_fdl_dbjob_" "$python_file" || true)
 
              # Write task line numbers and task names into output.txt
              while IFS= read -r line; do
                lineNumber=$(echo "$line" | cut -d: -f1)
                jobName=$(echo "$line" | cut -d: -f2- | sed "s/task_id='run_//" | sed "s/',//" | xargs)
                echo "$lineNumber $jobName" >> output.txt
              done <<< "$jobNames"
 
              # Process each task from output.txt
              echo "Processing tasks from output.txt"
              while IFS= read -r entry; do
                lineNumber=$(echo "$entry" | awk '{print $1}')
                task_name=$(echo "$entry" | awk '{print $2}')
                if [ "$task_name" ] && [ "$lineNumber" ]; then
                
                     echo "Processing task_name: $task_name on line $lineNumber"
                     curl   --connect-timeout 30 --retry 3 --retry-delay 5 \
                        --request GET "URL/api/2.1/jobs/list?name=${task_name}" \
                        --header "Authorization: Bearer TOKEN" > "apiresponse.json"
 
                     job_value=$(jq -r '.jobs[0].job_id // empty' "apiresponse.json")
                     job_name=$(jq -r '.jobs[0].settings.name // empty' "apiresponse.json")
                     if [ -z "$job_value" ] || [ -z "$job_name" ]; then
                          echo "Error: Missing job details for task_name: $task_name"
                          continue
                     fi
 
                    echo "Job Value: $job_value"
                    echo "Task Value: $job_name"
                     if [ "$task_name" == "$job_name" ]; then
                         echo "Updating job_id for $task_name on line $lineNumber in $python_file"
                         awk -v lineNum="$lineNumber" -v jobVal="$job_value" '
                         NR==lineNum+1 { sub(/job_id[[:space:]]*=.*/, "job_id=" jobVal ","); }
                         { print }
                         ' "$python_file" > temp.py && mv temp.py "$python_file"
                     else
                         echo "No match found for $task_name and $job_name"
                     fi
                else
                    echo "There is no job name that exists in DBR application for this $python_file"
                fi
              done < output.txt
            done
            rm -f apiresponse.json output.txt
            ls -ltrh
            
            #running for all script .py files
            cd ..
            cd scripts
            echo "Runing for all py files at scripts directory"
            python_files=$(find . -name "*.py")
            for python_file in $python_files; do
              echo "Processing file: $python_file"
 
              # Extract task_id lines from the current Python file
              jobNames=$(grep -n "task_id='run_fdl_dbjob_" "$python_file" || true)
              #this output file contains data from dags so clearing it here for fresh start for script side
              > output.txt
              while IFS= read -r line; do
                lineNumber=$(echo "$line" | cut -d: -f1)
                jobName=$(echo "$line" | cut -d: -f2- | sed "s/task_id='run_//" | sed "s/',//" | xargs)
                echo "$lineNumber $jobName" >> output.txt
              done <<< "$jobNames"
 
              # Process each task from output.txt
              echo "Processing tasks from output.txt"
              while IFS= read -r entry; do
                lineNumber=$(echo "$entry" | awk '{print $1}')
                task_name=$(echo "$entry" | awk '{print $2}')

                if [ "$task_name" ] && [ "$lineNumber" ]; then
                
                     echo "Processing task_name: $task_name on line $lineNumber"
                     curl   --connect-timeout 30 --retry 3 --retry-delay 5 \
                        --request GET "URL/api/2.1/jobs/list?name=${task_name}" \
                        --header "Authorization: Bearer TOKEN" > "apiresponse.json"
 
                     job_value=$(jq -r '.jobs[0].job_id // empty' "apiresponse.json")
                     job_name=$(jq -r '.jobs[0].settings.name // empty' "apiresponse.json")
                     if [ -z "$job_value" ] || [ -z "$job_name" ]; then
                          echo "Error: Missing job details for task_name: $task_name"
                          continue
                     fi
 
                    echo "Job Value: $job_value"
                    echo "Task Value: $job_name"
                     if [ "$task_name" == "$job_name" ]; then
                         echo "Updating job_id for $task_name on line $lineNumber in $python_file"
                         awk -v lineNum="$lineNumber" -v jobVal="$job_value" '
                         NR==lineNum+1 { sub(/job_id[[:space:]]*=.*/, "job_id=" jobVal ","); }
                         { print }
                         ' "$python_file" > temp.py && mv temp.py "$python_file"
                     else
                         echo "No match found for $task_name and $job_name"
                     fi
                else
                    echo "There is no job name that exists in DBR application for this $python_file file"
                fi
              done < output.txt
            done
            rm -f apiresponse.json output.txt
 
            # Sync the updated files to S3
            echo "checking current directory and Syncing files to S3"
            cd ..
            
            pwd
            ls -ltrh
            aws s3 sync ./ s3://testairflows3poc/dags/databricks/FDL --delete
            
            
        fi
